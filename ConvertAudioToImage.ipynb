{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvertAudioToImage.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bZPyAywQ2PKaQugWvmhaPdaiF9jNnsmN",
      "authorship_tag": "ABX9TyN6ho4J48Gp7fDs28pYyhZC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omiderfanmaneshit/speaker_identification-/blob/omid/ConvertAudioToImage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr896fKe1han",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U spafe\n",
        "!pip install --upgrade keras\n",
        "!pip install \"dask[complete]\"    # Install everything\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1h4ZtN4EoQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import librosa\n",
        "import scipy\n",
        "from scipy import signal\n",
        "from scipy.fftpack import fft\n",
        "from tqdm import tqdm\n",
        "from scipy.io import wavfile # get the api\n",
        "from sklearn import preprocessing\n",
        "import sklearn \n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import librosa\n",
        "import scipy\n",
        "from scipy import signal\n",
        "from scipy.fftpack import fft\n",
        "from scipy.io import wavfile # get the api\n",
        "from spafe.utils import vis\n",
        "from spafe.features.lpc import lpc, lpcc\n",
        "from spafe.features.mfcc import mfcc, imfcc\n",
        "from tqdm import tqdm\n",
        "import dask.dataframe as dd\n",
        "import dask\n",
        "\n",
        "# Dependencies\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm, tqdm_notebook; tqdm.pandas() # Progress bar\n",
        "from sklearn.metrics import label_ranking_average_precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Machine Learning\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.layers import (Dense, Bidirectional, LSTM, ELU,\n",
        "                          Dropout, LeakyReLU, Conv1D, BatchNormalization)\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "import csv\n",
        "import pywt\n",
        "import cv2 as cvlib\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6EuZsOoCBJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_data_all_gather(vect_in, out_min, out_max, percent_acceptation=80,\n",
        "                              not_clip_until_acceptation_time_factor=1.5):\n",
        "    # nb_dim = len(vect_in.shape)\n",
        "    percent_val = np.percentile(abs(vect_in).reshape((vect_in.shape[0], vect_in.shape[1] * vect_in.shape[2])),\n",
        "                                percent_acceptation, axis=1)\n",
        "    percent_val_matrix = not_clip_until_acceptation_time_factor * np.repeat(percent_val,\n",
        "                                                                            vect_in.shape[1] * vect_in.shape[2],\n",
        "                                                                            axis=0).reshape(\n",
        "        (vect_in.shape[0], vect_in.shape[1], vect_in.shape[2]))\n",
        "    matrix_clip = np.maximum(np.minimum(vect_in, percent_val_matrix), -percent_val_matrix)\n",
        "    return np.divide(matrix_clip, percent_val_matrix) * ((out_max - out_min) / 2) + (out_max + out_min) / 2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkPhPUuJCJaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_label_sound=np.zeros((24449,1))\n",
        "y_label_hot_sound=np.zeros((24449,10))\n",
        "n_fft = 1024 # frame length\n",
        "hop_length = 512"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56rTBPgNA5Eg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3da8463-458f-4009-e503-b5d06c66ffbe"
      },
      "source": [
        "folders_df = []\n",
        "arr = os.listdir('/content/drive/My Drive/dataScience/september exam/data/development')\n",
        "\n",
        "for folder in arr:\n",
        "  if folder == 'j':\n",
        "    path = '/content/drive/My Drive/dataScience/september exam/data/development/'+folder\n",
        "    onlyfiles = [f for f in os.listdir(path) if isfile(join(path, f))]\n",
        "    with tqdm(total=len(onlyfiles), desc=\"Adding Users\", bar_format=\"{l_bar}{bar} [ time left: {remaining} ]\") as pbar:\n",
        "      for p in onlyfiles:\n",
        "      \n",
        "        clipnoise, sample_rate = librosa.load(path +'/'+p,sr=24000)\n",
        "        scales = np.arange(1, 128)\n",
        "        waveletname = 'morl'\n",
        "\n",
        "        coeffnoise, freqnoise = pywt.cwt(clipnoise, scales, waveletname)\n",
        "\n",
        "        scalogramimg=cvlib.resize(coeffnoise, dsize=(224, 224), interpolation=cvlib.INTER_CUBIC)\n",
        "        stft = librosa.stft(clipnoise, n_fft=n_fft, hop_length=hop_length)\n",
        "        stft_magnitude, stft_phase = librosa.magphase(stft)\n",
        "        stft_magnitude_db = librosa.amplitude_to_db(stft_magnitude, ref=np.max)\n",
        "\n",
        "        spectrogramimg=cvlib.resize(stft_magnitude_db, dsize=(224, 224), interpolation=cvlib.INTER_CUBIC)\n",
        "\n",
        "        mfcc = librosa.feature.mfcc(y=clipnoise, sr=sample_rate, n_mfcc=200)\n",
        "        mfccimg=cvlib.resize(mfcc, dsize=(224, 224), interpolation=cvlib.INTER_CUBIC)\n",
        "\n",
        "        RGB_sound_VGG = np.zeros(( 224, 224, 3))\n",
        "\n",
        "        RGB_sound_VGG[ :, :, 0] = spectrogramimg\n",
        "        RGB_sound_VGG[ :, :, 1] = scalogramimg\n",
        "        RGB_sound_VGG[ :, :, 2] = mfccimg\n",
        "\n",
        "        cv2.imwrite('/content/drive/My Drive/dataScience/september exam/data/trainImg' +'/'+folder+'/'+p.split('.')[0]+'.png', RGB_sound_VGG)\n",
        "        pbar.update(1)\n",
        "\n",
        "      # RGB_sound_VGG = np.zeros(( 224, 224, 3))\n",
        "      # RGB_sound_VGG[ :, :, 0] = spectrogramimg\n",
        "      # RGB_sound_VGG[ :, :, 1] = scalogramimg\n",
        "      # RGB_sound_VGG[ :, :, 2] = mfccimg\n",
        "\n",
        "      # scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 255))\n",
        "      # scaler = scaler.fit(RGB_sound_VGG)\n",
        "      # X_0255 = scaler.transform(RGB_sound_VGG)\n",
        "\n",
        "      # cv2.imwrite('/content/drive/My Drive/dataScience/september exam/data/normalizedImg0255' +'/'+folder+'/'+p.split('.')[0]+'.png', X_0255)\n",
        "\n",
        "      # scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
        "      # RGB_sound_VGG = np.zeros(( 224, 224, 3))\n",
        "      # RGB_sound_VGG[ :, :, 0] = spectrogramimg\n",
        "      # RGB_sound_VGG[ :, :, 1] = scalogramimg\n",
        "      # RGB_sound_VGG[ :, :, 2] = mfccimg\n",
        "\n",
        "      \n",
        "      # scaler = scaler.fit(RGB_sound_VGG)\n",
        "      # X_01 = scaler.transform(RGB_sound_VGG)\n",
        "\n",
        "      # cv2.imwrite('/content/drive/My Drive/dataScience/september exam/data/normalizedImg01' +'/'+folder+'/'+p.split('.')[0]+'.png', X_01)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adding Users: 100%|██████████ [ time left: 00:00 ]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}